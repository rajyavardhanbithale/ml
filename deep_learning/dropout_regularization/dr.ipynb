{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3ed577-6a8b-4710-9594-6571acff8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc9abc5-e88e-49a8-8b0f-7675962171ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/sonar_dataset.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c70123-166e-450f-894e-0ba5b4a81df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.0474</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>0.0756</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>0.1734</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.2803</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>0.5020</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>0.7526</td>\n",
       "      <td>0.7298</td>\n",
       "      <td>0.6177</td>\n",
       "      <td>0.4946</td>\n",
       "      <td>0.4531</td>\n",
       "      <td>0.4099</td>\n",
       "      <td>0.4540</td>\n",
       "      <td>0.4124</td>\n",
       "      <td>0.3139</td>\n",
       "      <td>0.3194</td>\n",
       "      <td>0.3692</td>\n",
       "      <td>0.3776</td>\n",
       "      <td>0.4469</td>\n",
       "      <td>0.4777</td>\n",
       "      <td>0.4716</td>\n",
       "      <td>0.4664</td>\n",
       "      <td>0.3893</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.4064</td>\n",
       "      <td>0.3712</td>\n",
       "      <td>0.3863</td>\n",
       "      <td>0.2802</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.1303</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.1409</td>\n",
       "      <td>0.1916</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.1989</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.2863</td>\n",
       "      <td>0.3570</td>\n",
       "      <td>0.3980</td>\n",
       "      <td>0.4359</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>0.7435</td>\n",
       "      <td>0.8379</td>\n",
       "      <td>0.8641</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.9536</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>0.8962</td>\n",
       "      <td>0.7196</td>\n",
       "      <td>0.5462</td>\n",
       "      <td>0.3156</td>\n",
       "      <td>0.2525</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.2189</td>\n",
       "      <td>0.1533</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.2276</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.0773</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.2343</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.2934</td>\n",
       "      <td>0.3709</td>\n",
       "      <td>0.4309</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>0.5116</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>0.9104</td>\n",
       "      <td>0.8912</td>\n",
       "      <td>0.8189</td>\n",
       "      <td>0.6779</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.5651</td>\n",
       "      <td>0.5749</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.4467</td>\n",
       "      <td>0.4133</td>\n",
       "      <td>0.3743</td>\n",
       "      <td>0.3021</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.1790</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>0.1341</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>0.0836</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.1199</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.2042</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>0.2616</td>\n",
       "      <td>0.2097</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>0.3213</td>\n",
       "      <td>0.4327</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.5328</td>\n",
       "      <td>0.6057</td>\n",
       "      <td>0.6696</td>\n",
       "      <td>0.7476</td>\n",
       "      <td>0.8930</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.6701</td>\n",
       "      <td>0.4989</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.1416</td>\n",
       "      <td>0.2680</td>\n",
       "      <td>0.2630</td>\n",
       "      <td>0.3104</td>\n",
       "      <td>0.3392</td>\n",
       "      <td>0.2123</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.2203</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.0817</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.1168</td>\n",
       "      <td>0.1476</td>\n",
       "      <td>0.2118</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.1334</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.3685</td>\n",
       "      <td>0.4646</td>\n",
       "      <td>0.5418</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>0.8257</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>0.8949</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>0.6257</td>\n",
       "      <td>0.2184</td>\n",
       "      <td>0.2945</td>\n",
       "      <td>0.3645</td>\n",
       "      <td>0.5012</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.9361</td>\n",
       "      <td>0.8195</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.4513</td>\n",
       "      <td>0.3004</td>\n",
       "      <td>0.2674</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.2986</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4   ...      56      57      58      59  60\n",
       "106  0.0331  0.0423  0.0474  0.0818  0.0835  ...  0.0066  0.0044  0.0134  0.0092   M\n",
       "118  0.0363  0.0478  0.0298  0.0210  0.1409  ...  0.0066  0.0114  0.0073  0.0033   M\n",
       "162  0.0217  0.0152  0.0346  0.0346  0.0484  ...  0.0029  0.0022  0.0022  0.0032   M\n",
       "55   0.0201  0.0116  0.0123  0.0245  0.0547  ...  0.0045  0.0029  0.0008  0.0018   R\n",
       "168  0.0015  0.0186  0.0289  0.0195  0.0515  ...  0.0013  0.0010  0.0032  0.0047   M\n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c99a25-bce7-4e70-aee5-ef3ed5078d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[60] = df[60].map({'R':0,'M':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66094fb2-0977-4471-b4b8-335a9e433d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.1990</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.1891</td>\n",
       "      <td>0.2949</td>\n",
       "      <td>0.5343</td>\n",
       "      <td>0.6850</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>0.8220</td>\n",
       "      <td>0.7290</td>\n",
       "      <td>0.7352</td>\n",
       "      <td>0.7918</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.4898</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.2924</td>\n",
       "      <td>0.6255</td>\n",
       "      <td>0.8546</td>\n",
       "      <td>0.8966</td>\n",
       "      <td>0.7821</td>\n",
       "      <td>0.5168</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.3411</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.3702</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.0945</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.0615</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.0612</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>0.1063</td>\n",
       "      <td>0.1179</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.1591</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.1647</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>0.2816</td>\n",
       "      <td>0.4290</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>0.6119</td>\n",
       "      <td>0.5260</td>\n",
       "      <td>0.3677</td>\n",
       "      <td>0.2746</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.1952</td>\n",
       "      <td>0.1787</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.1762</td>\n",
       "      <td>0.2481</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>0.1206</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.3011</td>\n",
       "      <td>0.3361</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.1151</td>\n",
       "      <td>0.3404</td>\n",
       "      <td>0.5562</td>\n",
       "      <td>0.6379</td>\n",
       "      <td>0.6553</td>\n",
       "      <td>0.7384</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.6877</td>\n",
       "      <td>0.7325</td>\n",
       "      <td>0.7726</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.6092</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>0.1666</td>\n",
       "      <td>0.3749</td>\n",
       "      <td>0.3776</td>\n",
       "      <td>0.2106</td>\n",
       "      <td>0.5886</td>\n",
       "      <td>0.5628</td>\n",
       "      <td>0.2577</td>\n",
       "      <td>0.5245</td>\n",
       "      <td>0.6149</td>\n",
       "      <td>0.5123</td>\n",
       "      <td>0.3385</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.0630</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.1552</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>0.3089</td>\n",
       "      <td>0.3829</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>0.5335</td>\n",
       "      <td>0.5996</td>\n",
       "      <td>0.6728</td>\n",
       "      <td>0.7309</td>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>0.8991</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>0.9128</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>0.6018</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.1788</td>\n",
       "      <td>0.2373</td>\n",
       "      <td>0.2843</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.2715</td>\n",
       "      <td>0.3363</td>\n",
       "      <td>0.2546</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2898</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.7246</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.7032</td>\n",
       "      <td>0.7141</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.1823</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1552</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4   ...      56      57      58      59  60\n",
       "152  0.0131  0.0201  0.0045  0.0217  0.0230  ...  0.0030  0.0066  0.0029  0.0053   1\n",
       "72   0.0208  0.0186  0.0131  0.0211  0.0610  ...  0.0028  0.0019  0.0049  0.0023   0\n",
       "142  0.0526  0.0563  0.1219  0.1206  0.0246  ...  0.0132  0.0103  0.0364  0.0208   1\n",
       "121  0.0162  0.0041  0.0239  0.0441  0.0630  ...  0.0088  0.0036  0.0053  0.0030   1\n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  ...  0.0034  0.0079  0.0036  0.0048   1\n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43463312-deb8-48e0-98eb-4981cadcc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c09bf41-c82c-4b93-b29c-467922701917",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(columns=60,axis=1),df[60],test_size=0.2,random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b673c8-24bf-416c-ba9f-ac83450727d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5d9ab16-ba92-4ded-9a56-51a4a7c602ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajyavardhan/.local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5297 - loss: 0.6866\n",
      "Epoch 2/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5723 - loss: 0.6734 \n",
      "Epoch 3/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5849 - loss: 0.6718 \n",
      "Epoch 4/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6907 - loss: 0.6376 \n",
      "Epoch 5/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6786 - loss: 0.6227 \n",
      "Epoch 6/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7431 - loss: 0.5864 \n",
      "Epoch 7/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7544 - loss: 0.5404 \n",
      "Epoch 8/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8780 - loss: 0.4948 \n",
      "Epoch 9/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8028 - loss: 0.4623 \n",
      "Epoch 10/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8211 - loss: 0.4360 \n",
      "Epoch 11/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7468 - loss: 0.4397 \n",
      "Epoch 12/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8741 - loss: 0.3656 \n",
      "Epoch 13/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9015 - loss: 0.3322 \n",
      "Epoch 14/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8894 - loss: 0.3216 \n",
      "Epoch 15/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7984 - loss: 0.3930 \n",
      "Epoch 16/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8974 - loss: 0.2947 \n",
      "Epoch 17/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8892 - loss: 0.3162 \n",
      "Epoch 18/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9037 - loss: 0.2708 \n",
      "Epoch 19/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9294 - loss: 0.2493 \n",
      "Epoch 20/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9335 - loss: 0.2416 \n",
      "Epoch 21/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8779 - loss: 0.2704 \n",
      "Epoch 22/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.2419 \n",
      "Epoch 23/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8861 - loss: 0.2851 \n",
      "Epoch 24/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8956 - loss: 0.2498 \n",
      "Epoch 25/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.2360 \n",
      "Epoch 26/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.1748 \n",
      "Epoch 27/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.1676 \n",
      "Epoch 28/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9594 - loss: 0.1712 \n",
      "Epoch 29/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9332 - loss: 0.1557 \n",
      "Epoch 30/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.1340 \n",
      "Epoch 31/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1300 \n",
      "Epoch 32/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9233 - loss: 0.1721 \n",
      "Epoch 33/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9657 - loss: 0.1257 \n",
      "Epoch 34/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.1002 \n",
      "Epoch 35/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1286 \n",
      "Epoch 36/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0759 \n",
      "Epoch 37/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0676 \n",
      "Epoch 38/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9722 - loss: 0.0890 \n",
      "Epoch 39/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9889 - loss: 0.0891 \n",
      "Epoch 40/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0661 \n",
      "Epoch 41/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9889 - loss: 0.0832 \n",
      "Epoch 42/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9985 - loss: 0.0660 \n",
      "Epoch 43/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0518 \n",
      "Epoch 44/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0596 \n",
      "Epoch 45/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0592 \n",
      "Epoch 46/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9956 - loss: 0.0512 \n",
      "Epoch 47/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0406 \n",
      "Epoch 48/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0404 \n",
      "Epoch 49/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9979 - loss: 0.0353 \n",
      "Epoch 50/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0269 \n",
      "Epoch 51/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9979 - loss: 0.0282 \n",
      "Epoch 52/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0290 \n",
      "Epoch 53/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0214 \n",
      "Epoch 54/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0293 \n",
      "Epoch 55/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0361 \n",
      "Epoch 56/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0388 \n",
      "Epoch 57/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9916 - loss: 0.0391 \n",
      "Epoch 58/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9889 - loss: 0.0425 \n",
      "Epoch 59/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0228 \n",
      "Epoch 60/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0542 \n",
      "Epoch 61/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0165 \n",
      "Epoch 62/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0321 \n",
      "Epoch 63/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0219 \n",
      "Epoch 64/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9889 - loss: 0.0352 \n",
      "Epoch 65/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9946 - loss: 0.0214 \n",
      "Epoch 66/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0236 \n",
      "Epoch 67/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0199 \n",
      "Epoch 68/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0169  \n",
      "Epoch 69/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0115 \n",
      "Epoch 70/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0083 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x77a2a657d450>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(60,input_dim=60,activation='relu'),\n",
    "    keras.layers.Dense(40,activation='relu'),\n",
    "    keras.layers.Dense(20,activation='relu'),\n",
    "    keras.layers.Dense(10,activation='relu'), \n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train,y_train,epochs=70,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3daf4c3-699c-4556-bdad-cd6178cff718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8527 - loss: 0.7963  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7692375183105469, 0.8571428656578064]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81030dc2-3473-452e-9d40-949e0646c508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120    1\n",
       "124    1\n",
       "170    1\n",
       "205    1\n",
       "26     0\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a860e6e-3bcc-4b6b-89e4-257a365fa69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7236694 ],\n",
       "       [0.99650997],\n",
       "       [0.86167496],\n",
       "       [0.8937501 ],\n",
       "       [0.98447526]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)[:5]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f22cd653-afdd-4b89-859d-5bb9ea0bfd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_pred = [1 if x>0.5 else 0 for x in pred]\n",
    "lst_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed12e7d5-a035-4f90-960e-e0034f0a6310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajyavardhan/.local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5161 - loss: 0.7019\n",
      "Epoch 2/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3913 - loss: 0.7110 \n",
      "Epoch 3/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4859 - loss: 0.6960 \n",
      "Epoch 4/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4990 - loss: 0.6919 \n",
      "Epoch 5/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4898 - loss: 0.6857 \n",
      "Epoch 6/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5016 - loss: 0.6963 \n",
      "Epoch 7/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5052 - loss: 0.6950 \n",
      "Epoch 8/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 0.6784 \n",
      "Epoch 9/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 0.6818 \n",
      "Epoch 10/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5746 - loss: 0.6866 \n",
      "Epoch 11/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5946 - loss: 0.6732 \n",
      "Epoch 12/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5380 - loss: 0.6806 \n",
      "Epoch 13/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6214 - loss: 0.6742 \n",
      "Epoch 14/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5943 - loss: 0.6648 \n",
      "Epoch 15/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5926 - loss: 0.6679 \n",
      "Epoch 16/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5965 - loss: 0.6722 \n",
      "Epoch 17/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5856 - loss: 0.6700 \n",
      "Epoch 18/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6284 - loss: 0.6587 \n",
      "Epoch 19/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6281 - loss: 0.6468 \n",
      "Epoch 20/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5818 - loss: 0.6601 \n",
      "Epoch 21/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6587 - loss: 0.6319 \n",
      "Epoch 22/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6917 - loss: 0.6121 \n",
      "Epoch 23/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7259 - loss: 0.6019  \n",
      "Epoch 24/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7384 - loss: 0.5887 \n",
      "Epoch 25/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6891 - loss: 0.5862 \n",
      "Epoch 26/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7046 - loss: 0.5500 \n",
      "Epoch 27/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7083 - loss: 0.5965 \n",
      "Epoch 28/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7037 - loss: 0.5617 \n",
      "Epoch 29/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7780 - loss: 0.5083 \n",
      "Epoch 30/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7203 - loss: 0.5452 \n",
      "Epoch 31/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6391 - loss: 0.5825 \n",
      "Epoch 32/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7299 - loss: 0.5226 \n",
      "Epoch 33/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7817 - loss: 0.5077 \n",
      "Epoch 34/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7597 - loss: 0.5099 \n",
      "Epoch 35/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7869 - loss: 0.4691 \n",
      "Epoch 36/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7916 - loss: 0.4715  \n",
      "Epoch 37/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7412 - loss: 0.4675 \n",
      "Epoch 38/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7824 - loss: 0.5056 \n",
      "Epoch 39/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7966 - loss: 0.4569 \n",
      "Epoch 40/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7320 - loss: 0.5293 \n",
      "Epoch 41/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8077 - loss: 0.4480 \n",
      "Epoch 42/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.3900 \n",
      "Epoch 43/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7931 - loss: 0.4344 \n",
      "Epoch 44/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7982 - loss: 0.4706 \n",
      "Epoch 45/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8109 - loss: 0.4340 \n",
      "Epoch 46/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8406 - loss: 0.3995 \n",
      "Epoch 47/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8179 - loss: 0.4290  \n",
      "Epoch 48/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8141 - loss: 0.4345 \n",
      "Epoch 49/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8986 - loss: 0.3619 \n",
      "Epoch 50/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8435 - loss: 0.3615 \n",
      "Epoch 51/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8520 - loss: 0.3469 \n",
      "Epoch 52/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8748 - loss: 0.3649 \n",
      "Epoch 53/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8862 - loss: 0.3355 \n",
      "Epoch 54/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8356 - loss: 0.3872 \n",
      "Epoch 55/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8366 - loss: 0.4204 \n",
      "Epoch 56/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.3907 \n",
      "Epoch 57/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8061 - loss: 0.4537 \n",
      "Epoch 58/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9124 - loss: 0.2943 \n",
      "Epoch 59/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8662 - loss: 0.3033 \n",
      "Epoch 60/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8766 - loss: 0.2970 \n",
      "Epoch 61/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8906 - loss: 0.2763\n",
      "Epoch 62/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8647 - loss: 0.3120 \n",
      "Epoch 63/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8923 - loss: 0.3033 \n",
      "Epoch 64/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8512 - loss: 0.3406 \n",
      "Epoch 65/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8840 - loss: 0.3004 \n",
      "Epoch 66/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8782 - loss: 0.3072 \n",
      "Epoch 67/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8731 - loss: 0.3185 \n",
      "Epoch 68/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9194 - loss: 0.2876 \n",
      "Epoch 69/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8980 - loss: 0.2880 \n",
      "Epoch 70/70\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8503 - loss: 0.3166 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x77a265eac610>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_drop = keras.Sequential([\n",
    "    keras.layers.Dense(60,input_dim=60,activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(40,activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    \n",
    "    keras.layers.Dense(20,activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(10,activation='relu'),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    \n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_drop.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_drop.fit(x_train,y_train,epochs=70,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62e88e0d-8b72-465c-9a70-7f499411befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7738 - loss: 0.4434  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4090280532836914, 0.7857142686843872]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_drop.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "183e167e-7263-4843-916e-de79766d783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120    1\n",
       "124    1\n",
       "170    1\n",
       "205    1\n",
       "26     0\n",
       "97     1\n",
       "181    1\n",
       "72     0\n",
       "30     0\n",
       "201    1\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5dc03e7-2198-463f-adb7-980573107db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.35479674],\n",
       "       [0.8259679 ],\n",
       "       [0.36867777],\n",
       "       [0.9261304 ],\n",
       "       [0.81610334],\n",
       "       [0.03326457],\n",
       "       [0.99729687],\n",
       "       [0.10069368],\n",
       "       [0.12669288],\n",
       "       [0.9668528 ]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_drop.predict(x_test)[:10]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7963d135-e190-4691-ba9c-bb959da59e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 1, 0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_pred = [1 if x>0.5 else 0 for x in pred]\n",
    "lst_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
